{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# Using ESOL data\n",
    "\n",
    "import pandas as pd\n",
    "data_train = pd.read_csv('train_data.csv')\n",
    "data_train = data_train[['smiles','measured log solubility in mols per litre']]\n",
    "data_train.columns = ['smiles','solubility']\n",
    "\n",
    "data_test = pd.read_csv('test_data.csv')\n",
    "data_test = data_test[['smiles','measured log solubility in mols per litre']]\n",
    "data_test.columns = ['smiles','solubility']\n",
    "\n",
    "data_val= pd.read_csv('valid_data.csv')\n",
    "data_val = data_val[['smiles','measured log solubility in mols per litre']]\n",
    "data_val.columns = ['smiles','solubility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_graph(smiles, solubility):\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atoms = mol.GetAtoms()\n",
    "    \n",
    "    # atoms - nodes\n",
    "    # node feature vector, just a single feature atomic number\n",
    "    atm_numbers = [atom.GetAtomicNum() for atom in atoms]\n",
    "    atomic_numbers = torch.tensor(atm_numbers, dtype=torch.long).unsqueeze(1)\n",
    "    \n",
    "    # bonds - edges\n",
    "    edge_index = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_index += [[i, j], [j, i]]\n",
    "        \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "    sol_target = torch.tensor(solubility, dtype=torch.float)\n",
    "    \n",
    "    return Data(x=atomic_numbers, edge_index=edge_index, y=sol_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhrup\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "class ESOLData(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data_list = [to_graph(j.smiles, j.solubility) for i,j in data.iterrows()]\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_data = ESOLData(data_train)\n",
    "test_data = ESOLData(data_test)\n",
    "val_data = ESOLData(data_val)\n",
    "\n",
    "# load data to create mini batches\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_layer=64):\n",
    "        # super(GNNModel, self).__init__(hidden_layer)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(1, hidden_layer)\n",
    "        self.conv2 = GCNConv(hidden_layer, hidden_layer)\n",
    "        # self.conv3 = GCNConv(hidden_layer, hidden_layer)\n",
    "        self.lin = nn.Linear(hidden_layer,1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n",
    "        \n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        # x = torch.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNModel()\n",
    "opti = Adam(model.parameters(), lr=0.0005)\n",
    "loss_fx = nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        opti.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fx(out, data.y)\n",
    "        loss.backward()\n",
    "        opti.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            out = model(data)\n",
    "            loss = loss_fx(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhrup\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\dhrup\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\dhrup\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 11.2758 | Test Loss: 14.1498\n",
      "Epoch 002 | Train Loss: 7.3335 | Test Loss: 9.6821\n",
      "Epoch 003 | Train Loss: 5.4856 | Test Loss: 6.9601\n",
      "Epoch 004 | Train Loss: 4.9911 | Test Loss: 6.4547\n",
      "Epoch 005 | Train Loss: 5.0145 | Test Loss: 6.4956\n",
      "Epoch 006 | Train Loss: 5.0098 | Test Loss: 6.3579\n",
      "Epoch 007 | Train Loss: 5.1782 | Test Loss: 6.4147\n",
      "Epoch 008 | Train Loss: 4.9459 | Test Loss: 6.6341\n",
      "Epoch 009 | Train Loss: 4.9192 | Test Loss: 6.4864\n",
      "Epoch 010 | Train Loss: 4.8949 | Test Loss: 6.5615\n",
      "Epoch 011 | Train Loss: 4.9028 | Test Loss: 6.4182\n",
      "Epoch 012 | Train Loss: 4.8977 | Test Loss: 6.6631\n",
      "Epoch 013 | Train Loss: 5.2108 | Test Loss: 6.3807\n",
      "Epoch 014 | Train Loss: 4.8535 | Test Loss: 6.4427\n",
      "Epoch 015 | Train Loss: 5.0982 | Test Loss: 6.4421\n",
      "Epoch 016 | Train Loss: 5.0502 | Test Loss: 6.6380\n",
      "Epoch 017 | Train Loss: 5.0587 | Test Loss: 6.4442\n",
      "Epoch 018 | Train Loss: 4.8772 | Test Loss: 6.3088\n",
      "Epoch 019 | Train Loss: 5.1883 | Test Loss: 6.6644\n",
      "Epoch 020 | Train Loss: 4.8831 | Test Loss: 6.6941\n",
      "Epoch 021 | Train Loss: 4.8135 | Test Loss: 6.3912\n",
      "Epoch 022 | Train Loss: 4.9870 | Test Loss: 6.1992\n",
      "Epoch 023 | Train Loss: 4.8020 | Test Loss: 6.5915\n",
      "Epoch 024 | Train Loss: 4.7645 | Test Loss: 6.1865\n",
      "Epoch 025 | Train Loss: 4.7463 | Test Loss: 6.2443\n",
      "Epoch 026 | Train Loss: 4.8758 | Test Loss: 6.3904\n",
      "Epoch 027 | Train Loss: 4.7095 | Test Loss: 6.1889\n",
      "Epoch 028 | Train Loss: 4.9814 | Test Loss: 6.1971\n",
      "Epoch 029 | Train Loss: 4.7461 | Test Loss: 6.4096\n",
      "Epoch 030 | Train Loss: 4.6679 | Test Loss: 6.2126\n",
      "Epoch 031 | Train Loss: 4.7612 | Test Loss: 6.2002\n",
      "Epoch 032 | Train Loss: 4.6059 | Test Loss: 6.1643\n",
      "Epoch 033 | Train Loss: 4.6535 | Test Loss: 6.1285\n",
      "Epoch 034 | Train Loss: 4.9035 | Test Loss: 6.1137\n",
      "Epoch 035 | Train Loss: 4.7181 | Test Loss: 6.0790\n",
      "Epoch 036 | Train Loss: 4.6721 | Test Loss: 6.1195\n",
      "Epoch 037 | Train Loss: 4.6226 | Test Loss: 6.0615\n",
      "Epoch 038 | Train Loss: 4.5625 | Test Loss: 6.0824\n",
      "Epoch 039 | Train Loss: 4.5462 | Test Loss: 6.0750\n",
      "Epoch 040 | Train Loss: 4.6147 | Test Loss: 6.0381\n",
      "Epoch 041 | Train Loss: 4.6608 | Test Loss: 6.0105\n",
      "Epoch 042 | Train Loss: 4.6607 | Test Loss: 5.9988\n",
      "Epoch 043 | Train Loss: 4.6151 | Test Loss: 6.0935\n",
      "Epoch 044 | Train Loss: 4.5610 | Test Loss: 6.0725\n",
      "Epoch 045 | Train Loss: 4.6821 | Test Loss: 5.7290\n",
      "Epoch 046 | Train Loss: 4.8863 | Test Loss: 6.0082\n",
      "Epoch 047 | Train Loss: 4.6034 | Test Loss: 5.8998\n",
      "Epoch 048 | Train Loss: 4.5137 | Test Loss: 5.9811\n",
      "Epoch 049 | Train Loss: 4.5123 | Test Loss: 5.8422\n",
      "Epoch 050 | Train Loss: 4.4090 | Test Loss: 5.7659\n",
      "Epoch 051 | Train Loss: 4.3765 | Test Loss: 5.6967\n",
      "Epoch 052 | Train Loss: 4.3926 | Test Loss: 5.7159\n",
      "Epoch 053 | Train Loss: 4.3272 | Test Loss: 5.9279\n",
      "Epoch 054 | Train Loss: 4.4111 | Test Loss: 5.3454\n",
      "Epoch 055 | Train Loss: 4.3109 | Test Loss: 5.8321\n",
      "Epoch 056 | Train Loss: 4.6275 | Test Loss: 5.6372\n",
      "Epoch 057 | Train Loss: 4.4951 | Test Loss: 5.7278\n",
      "Epoch 058 | Train Loss: 4.3817 | Test Loss: 5.5679\n",
      "Epoch 059 | Train Loss: 4.3394 | Test Loss: 5.9406\n",
      "Epoch 060 | Train Loss: 4.3113 | Test Loss: 5.4949\n",
      "Epoch 061 | Train Loss: 4.3790 | Test Loss: 5.5201\n",
      "Epoch 062 | Train Loss: 4.3502 | Test Loss: 5.6804\n",
      "Epoch 063 | Train Loss: 4.2483 | Test Loss: 5.5138\n",
      "Epoch 064 | Train Loss: 4.3461 | Test Loss: 5.5968\n",
      "Epoch 065 | Train Loss: 4.3445 | Test Loss: 5.1871\n",
      "Epoch 066 | Train Loss: 4.4415 | Test Loss: 5.4289\n",
      "Epoch 067 | Train Loss: 4.3951 | Test Loss: 5.5507\n",
      "Epoch 068 | Train Loss: 4.2089 | Test Loss: 5.3757\n",
      "Epoch 069 | Train Loss: 4.2723 | Test Loss: 5.5692\n",
      "Epoch 070 | Train Loss: 4.2244 | Test Loss: 5.4372\n",
      "Epoch 071 | Train Loss: 4.3274 | Test Loss: 5.5166\n",
      "Epoch 072 | Train Loss: 4.2923 | Test Loss: 5.3463\n",
      "Epoch 073 | Train Loss: 4.2625 | Test Loss: 5.4078\n",
      "Epoch 074 | Train Loss: 4.3151 | Test Loss: 5.2363\n",
      "Epoch 075 | Train Loss: 4.5461 | Test Loss: 5.3554\n",
      "Epoch 076 | Train Loss: 4.2298 | Test Loss: 5.6209\n",
      "Epoch 077 | Train Loss: 4.3084 | Test Loss: 5.3629\n",
      "Epoch 078 | Train Loss: 4.2463 | Test Loss: 5.4622\n",
      "Epoch 079 | Train Loss: 4.2469 | Test Loss: 5.5615\n",
      "Epoch 080 | Train Loss: 4.2764 | Test Loss: 5.4537\n",
      "Epoch 081 | Train Loss: 4.2303 | Test Loss: 5.4926\n",
      "Epoch 082 | Train Loss: 4.2208 | Test Loss: 5.3399\n",
      "Epoch 083 | Train Loss: 4.2177 | Test Loss: 5.3413\n",
      "Epoch 084 | Train Loss: 4.2176 | Test Loss: 5.7215\n",
      "Epoch 085 | Train Loss: 4.2529 | Test Loss: 5.3568\n",
      "Epoch 086 | Train Loss: 4.3683 | Test Loss: 5.3214\n",
      "Epoch 087 | Train Loss: 4.2021 | Test Loss: 5.4674\n",
      "Epoch 088 | Train Loss: 4.2140 | Test Loss: 5.4178\n",
      "Epoch 089 | Train Loss: 4.2922 | Test Loss: 5.3848\n",
      "Epoch 090 | Train Loss: 4.2162 | Test Loss: 5.3570\n",
      "Epoch 091 | Train Loss: 4.4310 | Test Loss: 5.3872\n",
      "Epoch 092 | Train Loss: 4.2284 | Test Loss: 5.3282\n",
      "Epoch 093 | Train Loss: 4.2158 | Test Loss: 5.4580\n",
      "Epoch 094 | Train Loss: 4.3510 | Test Loss: 5.2068\n",
      "Epoch 095 | Train Loss: 4.2274 | Test Loss: 5.5750\n",
      "Epoch 096 | Train Loss: 4.2469 | Test Loss: 5.3179\n",
      "Epoch 097 | Train Loss: 4.3241 | Test Loss: 5.5109\n",
      "Epoch 098 | Train Loss: 4.2822 | Test Loss: 5.5287\n",
      "Epoch 099 | Train Loss: 4.2276 | Test Loss: 5.3412\n",
      "Epoch 100 | Train Loss: 4.1759 | Test Loss: 5.4798\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "    \n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimental Data -> [-2.12, -3.401]\n",
      "Predicted data\n",
      "tensor([[-2.8470]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.8418]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Validate first 2 rows of validation set\n",
    "val_test = data_val.head(2)\n",
    "print(\"Experimental Data ->\",[a.solubility for i,a in val_test.iterrows()])\n",
    "val_test1 = ESOLData(val_test)\n",
    "val_test2 = DataLoader(val_test1)\n",
    "print(\"Predicted data\")\n",
    "for data in val_test2:\n",
    "    out = model(data)\n",
    "    print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
